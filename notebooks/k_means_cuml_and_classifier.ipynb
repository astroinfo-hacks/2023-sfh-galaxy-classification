{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "723355ef-131d-4d44-8cce-896e97c76fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13fe93b4-af50-400b-a30c-535b5ac11731",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfswork/rech/owt/commun/galaxy_classification/.local_rapids/lib/python3.10/site-packages/tslearn/bases/bases.py:15: UserWarning: h5py not installed, hdf5 features will not be supported.\n",
      "Install h5py to use hdf5 features: http://docs.h5py.org/\n",
      "  warn(h5py_msg)\n"
     ]
    }
   ],
   "source": [
    "import tslearn\n",
    "import pandas as pd\n",
    "import cupy\n",
    "import numpy\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import re\n",
    "from astropy.table import Table\n",
    "from cuml.multiclass import OneVsRestClassifier\n",
    "from cuml.svm import LinearSVC\n",
    "from cuml.linear_model import LogisticRegression\n",
    "from cuml.metrics import accuracy_score\n",
    "from cuml.cluster import KMeans as KMeans\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def read_sfh_binned_data(dirname,filename):\n",
    "    result = re.search('binned_SFHs-(.*)levels-JWST_(.*).txt', filename)\n",
    "    if result:\n",
    "        levels = int(result.group(1))\n",
    "        z_bins = result.group(2)\n",
    "        IDs, SFH_lev = read_sfh_data(dirname,filename,levels)\n",
    "        return IDs, SFH_lev, levels, z_bins\n",
    "    else:\n",
    "        print(\"Filename doesn't contain levels or z_bins\")\n",
    "\n",
    "def read_sfh_data(dirname,filename,levels):\n",
    "    data = join(dirname, filename)\n",
    "    if isfile(data):\n",
    "        df = pd.read_csv(data,sep='\\t')    \n",
    "        levels=df.columns[2:levels+2]\n",
    "        SFH_lev=df[levels].values\n",
    "        IDs = df['id_L19'].astype(int)\n",
    "        return IDs, SFH_lev\n",
    "    \n",
    "def run_kmeans(x,num_clusters):\n",
    "    x = TimeSeriesScalerMeanVariance().fit_transform(x)        \n",
    "    X_train=cupy.asarray(x)\n",
    "    kmeans = KMeans(n_clusters=num_clusters, max_iter=300, init='scalable-k-means++')\n",
    "    kmeans.fit(X_train)\n",
    "    cluster = cupy.asnumpy(kmeans.fit_predict(X_train))\n",
    "    cluster_centers = cupy.asnumpy(kmeans.cluster_centers_)\n",
    "    return x, cluster, cluster_centers\n",
    "\n",
    "def get_sfh_cluster(dirname,filename,num_clusters):\n",
    "    df_cluster = pd.DataFrame()\n",
    "    IDs, SFH_lev, _, _ = read_sfh_binned_data(dirname,filename)\n",
    "    _, cluster, _ = run_kmeans(SFH_lev,num_clusters)\n",
    "    df_cluster['ID'] = IDs\n",
    "    df_cluster['cluster'] = cluster\n",
    "    return df_cluster\n",
    "\n",
    "def get_photometry(filepath):\n",
    "    data=Table.read(filepath)\n",
    "    df_photometry=data.to_pandas()\n",
    "    return df_photometry\n",
    "    \n",
    "def get_flux_cluster(df_cluster,df_photometry):\n",
    "    df_photometry = df_photometry.merge(df_cluster, on= 'ID')\n",
    "    df_flux = df_photometry[['ID','u_FLUX', 'gHSC_FLUX', 'rHSC_FLUX', 'iHSC_FLUX', 'zHSC_FLUX', 'cluster']]\n",
    "    return df_flux\n",
    "    \n",
    "def train_classifier(dataset, classes):\n",
    "    features = dataset[[\"u_FLUX\",\"gHSC_FLUX\",\"rHSC_FLUX\",\"iHSC_FLUX\",\"zHSC_FLUX\"]]\n",
    "    labels = dataset[\"cluster\"]\n",
    "    X_data = features.to_numpy()\n",
    "    Y_data= labels.to_numpy()\n",
    "    \n",
    "    #Create a scaler model that is fit on the input data.\n",
    "    scaler = StandardScaler().fit(X_data)\n",
    "\n",
    "    #Scale the numeric feature variables\n",
    "    X_data = scaler.transform(X_data)\n",
    "\n",
    "    #Convert target variable as a one-hot-encoding array\n",
    "    Y_data = label_binarize(Y_data,classes=classes)\n",
    "    \n",
    "    #Split training and test data\n",
    "    X_train,X_test,Y_train,Y_test = train_test_split( X_data, Y_data, train_size=0.75)\n",
    "\n",
    "    X_test,X_val,Y_test,Y_val = train_test_split( X_test, Y_test, test_size=0.5)\n",
    "\n",
    "    classifier = OneVsRestClassifier(LinearSVC(probability=True))\n",
    "    #classifier = OneVsRestClassifier(LogisticRegression())\n",
    "    classifier.fit(X_train, Y_train)\n",
    "    Y_test_hat = classifier.predict(X_test)\n",
    "    \n",
    "    score = accuracy_score(Y_test, Y_test_hat)\n",
    "\n",
    "    return classifier, score\n",
    "\n",
    "def run_pipeline(SFH_dirname,SFH_filename,FITS_filepath,num_clusters=6):\n",
    "    df_photometry = get_photometry(FITS_filepath)\n",
    "    df_cluster = get_sfh_cluster(SFH_dirname,SFH_filename,num_clusters)\n",
    "    df_flux = get_flux_cluster(df_cluster,df_photometry)\n",
    "    classes = []\n",
    "    for i in range(num_clusters):\n",
    "        classes.append(i)\n",
    "    classifier, score = train_classifier(df_flux, classes)\n",
    "    return classifier, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bd05010-45e9-44eb-8c64-a46f7f81a7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "myDir = \"/gpfswork/rech/owt/commun/galaxy_classification/2023-sfh-galaxy-classification/data/binned_SFHs/\"\n",
    "mySFH = 'binned_SFHs-11levels-JWST_z_0.5-1.0.txt'\n",
    "myFITS = '/gpfswork/rech/owt/commun/galaxy_classification/2023-sfh-galaxy-classification/data/Horizon_AGN-COSMOS_like/HorizonAGN_COSMOS-Web_v2.0_witherr.fits'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa81216c-20b1-4029-b584-36cd9bc96b9e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ab0deb-bbc5-4e30-b06c-44d4617d6a1a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 1. Extract labels from Input Data\n",
    "\n",
    "Get the files list and the following labels:\n",
    "1. SFH binning resolution (levels)\n",
    "2. Redshift binning (z_start-end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997c2afd-fbe1-43ff-8754-35e2cf3ca993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 8, 9, 10, 11] ['z_0.5-1.0', 'z_1.0-1.5', 'z_1.5-2.0', 'z_2.0-2.5', 'z_2.5-3.0', 'z_3.0-3.5', 'z_3.5-4.0']\n"
     ]
    }
   ],
   "source": [
    "myFiles = sorted(listdir(myDir))\n",
    "files = [f for f in myFiles if isfile(join(myDir, f))]\n",
    "z_bins_list = []\n",
    "level_bins_list = []\n",
    "for data in files:\n",
    "    result = re.search('binned_SFHs-(.*)levels-JWST_(.*).txt', data)\n",
    "    level_bins = int(result.group(1))\n",
    "    z_bins = result.group(2)\n",
    "    z_bins_list.append(z_bins)\n",
    "    level_bins_list.append(level_bins)\n",
    "    \n",
    "z_bins_list=sorted(list(set(z_bins_list)))\n",
    "level_bins_list=sorted(list(set(level_bins_list)))\n",
    "print(level_bins_list, z_bins_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce239369-1b50-4cd3-9612-e6862bb94a51",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 2. Plot cluster centers for each file\n",
    "\n",
    "Pipeline:\n",
    "1. Preprocessing unsing TimeSeriesScalerMeanVariance from tslearn\n",
    "2. Clustering using Kmeans from cuml\n",
    "3. Plot cluster centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd07142-e1b6-45a3-bc8c-eba3624f3448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "908e393503f24defb464c1f0f39ed4f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "outDir = \"cluster_centers\"\n",
    "\n",
    "num_clusters = 6\n",
    "seed = 0\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "for data in tqdm(files):\n",
    "    _, x, level_bins, z_bins = read_sfh_binned_data(myDir, data)    \n",
    "    x, y_pred, cluster_centers = run_kmeans(x,num_clusters)                                        \n",
    "    fig = plt.figure(figsize=(12, 18), dpi=150)\n",
    "    \n",
    "    for yi in range(6):\n",
    "        plt.subplot(6, 3, yi + 1)\n",
    "        plt.plot(cluster_centers[yi].ravel(), \"r-\")\n",
    "        plt.xlim(0, x.shape[1])\n",
    "        plt.ylim(-4, 4)\n",
    "        plt.text(0.55, 0.85, 'Cluster %d' % (yi + 1), transform=plt.gca().transAxes)\n",
    "        if yi == 1:\n",
    "            plt.title(str(level_bins)+\"levels \"+z_bins)\n",
    "\n",
    "    plt.tight_layout()    \n",
    "    fig.savefig(join(outDir,data+'.png'))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3091b75d-455f-46e7-b3ab-15ce71863fbf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 3. Plot cluster centers for each SFH binning resolution\n",
    "\n",
    "Pipeline:\n",
    "1. Preprocessing unsing TimeSeriesScalerMeanVariance from tslearn\n",
    "2. Clustering using Kmeans from cuml\n",
    "3. Plot cluster centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33751966-bbd2-430a-86da-412838d9bd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "outDir = \"cluster_centers_2\"\n",
    "\n",
    "num_clusters = 6\n",
    "seed = 0\n",
    "numpy.random.seed(seed)\n",
    "pre_level_bins = int(10)\n",
    "plt_id = 0                                         \n",
    "fig = plt.figure(figsize=(12, 18), dpi=150)\n",
    "\n",
    "for data in tqdm(files):\n",
    "    _, x, level_bins, z_bins = read_sfh_binned_data(myDir, data)    \n",
    "    x, y_pred, cluster_centers = run_kmeans(x,num_clusters)\n",
    "    \n",
    "    if pre_level_bins != level_bins:\n",
    "        plt.tight_layout()\n",
    "        #plt.show()\n",
    "        fig.savefig(join(outDir,str(pre_level_bins)+'levels.png'))\n",
    "        plt.close()\n",
    "        plt_id = 0    \n",
    "        fig = plt.figure(figsize=(12, 18), dpi=150)\n",
    "        \n",
    "    plt_id = plt_id + 1\n",
    "    plt.subplot(8,4,plt_id)\n",
    "    for yi in range(6):\n",
    "        plt.plot(cluster_centers[yi].ravel(),label='Cluster centers %d' % (yi + 1))    \n",
    "    plt.text(0.55, 0.85,z_bins, transform=plt.gca().transAxes)\n",
    "    plt.xlim(0, x.shape[1])\n",
    "    plt.ylim(-4, 4)\n",
    "    if plt_id == 1:\n",
    "        plt.title('Cluster centers '+str(level_bins)+'levels')\n",
    "    \n",
    "    pre_level_bins = level_bins\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.show()\n",
    "fig.savefig(join(outDir,str(pre_level_bins)+'levels.png'))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b68076a-9b7c-47bf-91a1-d2bfd2ff4107",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 4. Plot cluster centers for each redshift bins\n",
    "\n",
    "Pipeline:\n",
    "1. Preprocessing unsing TimeSeriesScalerMeanVariance from tslearn\n",
    "2. Clustering using Kmeans from cuml\n",
    "3. Plot cluster centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd49955b-8b9b-45e5-98ac-8039a1200bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "outDir = \"cluster_centers_3\"\n",
    "\n",
    "num_clusters = 6\n",
    "seed = 0\n",
    "numpy.random.seed(seed)\n",
    "pre_z_bins = \"z_0.5-1.0\"\n",
    "plt_id = 0    \n",
    "fig = plt.figure(figsize=(12, 18), dpi=150)\n",
    "\n",
    "for z_bins in tqdm(z_bins_list):\n",
    "    for level in range(7,12):\n",
    "        data = 'binned_SFHs-'+str(level)+'levels-JWST_'+z_bins+'.txt'\n",
    "        _, x, level_bins, z_bins = read_sfh_binned_data(myDir, data)    \n",
    "        x, y_pred, cluster_centers = run_kmeans(x,num_clusters)   \n",
    "\n",
    "        if pre_z_bins != z_bins:\n",
    "            plt.tight_layout()\n",
    "            #plt.show()\n",
    "            fig.savefig(join(outDir,pre_z_bins+'.png'))\n",
    "            plt.close()\n",
    "            plt_id = 0    \n",
    "            fig = plt.figure(figsize=(12, 18), dpi=150)\n",
    "\n",
    "        plt_id = plt_id + 1\n",
    "        plt.subplot(6,3,plt_id)\n",
    "        for yi in range(6):\n",
    "            plt.plot(cluster_centers[yi].ravel(),label='Cluster centers %d' % (yi + 1))    \n",
    "        plt.text(0.55, 0.85,str(level)+'levels', transform=plt.gca().transAxes)\n",
    "        plt.xlim(0, x.shape[1])\n",
    "        plt.ylim(-4, 4)\n",
    "        if plt_id == 1:\n",
    "            plt.title('Cluster centers '+z_bins)\n",
    "\n",
    "        pre_z_bins = z_bins\n",
    "                \n",
    "plt.tight_layout()\n",
    "#plt.show()\n",
    "fig.savefig(join(outDir,pre_z_bins+'.png'))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860933fb-de16-4466-90ee-ed2ddaf86dd5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 5. Plot cluster histogram for each SFH binning resolution\n",
    "\n",
    "Pipeline:\n",
    "1. Preprocessing unsing TimeSeriesScalerMeanVariance from tslearn\n",
    "2. Clustering using Kmeans from cuml\n",
    "3. Plot cluster histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7f855f-0246-4c39-b960-55d30cbfc9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "outDir = \"hist_2\"\n",
    "\n",
    "num_clusters = 6\n",
    "seed = 0\n",
    "numpy.random.seed(seed)\n",
    "pre_level_bins = int(10)\n",
    "plt_id = 0    \n",
    "fig = plt.figure(figsize=(12, 18), dpi=150)\n",
    "\n",
    "for data in tqdm(files):\n",
    "    _, x, level_bins, z_bins = read_sfh_binned_data(myDir, data)    \n",
    "    x, y_pred, cluster_centers = run_kmeans(x,num_clusters)       \n",
    "    \n",
    "    if pre_level_bins != level_bins:\n",
    "        plt.tight_layout()\n",
    "        #plt.show()\n",
    "        fig.savefig(join(outDir,str(pre_level_bins)+'levels.png'))\n",
    "        plt.close()\n",
    "        plt_id = 0    \n",
    "        fig = plt.figure(figsize=(12, 18), dpi=150)        \n",
    "    \n",
    "    plt_id = plt_id + 1\n",
    "    plt.subplot(8,4,plt_id)\n",
    "    plt.hist(y_pred)   \n",
    "    plt.text(0.55, 0.85,z_bins, transform=plt.gca().transAxes)\n",
    "    if plt_id == 1:\n",
    "        plt.title('Cluster histogram '+str(level_bins)+'levels')\n",
    "        \n",
    "    pre_level_bins = level_bins\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.show()\n",
    "fig.savefig(join(outDir,str(pre_level_bins)+'levels.png'))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8ab5a4-727a-451e-89fe-f6d10a6a9150",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 6. Plot cluster histogram for each redshift bins\n",
    "\n",
    "Pipeline:\n",
    "1. Preprocessing unsing TimeSeriesScalerMeanVariance from tslearn\n",
    "2. Clustering using Kmeans from cuml\n",
    "3. Plot cluster histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dedfca-a322-4f0e-9fc8-aa5f8c14bad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "outDir = \"hist_3\"\n",
    "\n",
    "num_clusters = 6\n",
    "seed = 0\n",
    "numpy.random.seed(seed)\n",
    "pre_z_bins = \"z_0.5-1.0\"\n",
    "plt_id = 0    \n",
    "fig = plt.figure(figsize=(12, 18), dpi=150)\n",
    "\n",
    "for z_bins in tqdm(z_bins_list):\n",
    "    for level in range(7,12):\n",
    "        data = 'binned_SFHs-'+str(level)+'levels-JWST_'+z_bins+'.txt'\n",
    "        _, x, level_bins, z_bins = read_sfh_binned_data(myDir, data)    \n",
    "        x, y_pred, cluster_centers = run_kmeans(x,num_clusters)\n",
    "\n",
    "        if pre_z_bins != z_bins:\n",
    "            plt.tight_layout()\n",
    "            #plt.show()\n",
    "            fig.savefig(join(outDir,pre_z_bins+'.png'))\n",
    "            plt.close()\n",
    "            plt_id = 0    \n",
    "            fig = plt.figure(figsize=(12, 18), dpi=150)\n",
    "\n",
    "        plt_id = plt_id + 1\n",
    "        plt.subplot(6,3,plt_id)\n",
    "        plt.hist(y_pred)   \n",
    "        plt.text(0.55, 0.85,str(level)+'levels', transform=plt.gca().transAxes)\n",
    "        if plt_id == 1:\n",
    "            plt.title('Cluster histogram '+z_bins)\n",
    "\n",
    "        pre_z_bins = z_bins\n",
    "                \n",
    "plt.tight_layout()\n",
    "#plt.show()\n",
    "fig.savefig(join(outDir,pre_z_bins+'.png'))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d508df0-0cec-44d6-916a-7a4cf3436eec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 7. Plot cluster centers and histogram for each redshift bins at a given SFH resolution\n",
    "\n",
    "Pipeline:\n",
    "1. Preprocessing unsing TimeSeriesScalerMeanVariance from tslearn\n",
    "2. Clustering using Kmeans from cuml\n",
    "3. Plot cluster centers and histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77a3b73-3b83-4f38-a618-2bfc06e88fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "outDir = \"cluster_centers_hist\"\n",
    "\n",
    "num_clusters = 6\n",
    "seed = 0\n",
    "numpy.random.seed(seed)\n",
    "plt_id = 0    \n",
    "fig = plt.figure(figsize=(12, 18), dpi=150)\n",
    "\n",
    "for z_bins in tqdm(z_bins_list):\n",
    "    data = 'binned_SFHs-11levels-JWST_'+z_bins+'.txt'\n",
    "    _, x, level_bins, z_bins = read_sfh_binned_data(myDir, data)    \n",
    "    x, y_pred, cluster_centers = run_kmeans(x,num_clusters)\n",
    "\n",
    "    plt_id = plt_id + 1\n",
    "    plt.subplot(14,7,plt_id)\n",
    "    for yi in range(6):\n",
    "        plt.plot(cluster_centers[yi].ravel(),label='Cluster centers %d' % (yi + 1))    \n",
    "    plt.text(0.55, 0.85,z_bins, transform=plt.gca().transAxes)\n",
    "    plt.xlim(0, x.shape[1])\n",
    "    plt.ylim(-4, 4)\n",
    "\n",
    "    plt.subplot(14,7,plt_id+7)\n",
    "    plt.hist(y_pred)   \n",
    "    plt.text(0.55, 0.85,z_bins, transform=plt.gca().transAxes)\n",
    "                \n",
    "plt.tight_layout()\n",
    "#plt.show()\n",
    "fig.savefig(join(outDir,'11levels.png'))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a41acf-2b5d-4ce5-88f3-11b46f7c030a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc735e6-6e06-41a8-80e9-a36efc38128d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Pipeline example of classifier\n",
    "\n",
    "Pipeline:\n",
    "1. Get fluxes and clusters for different number of clusters\n",
    "2. Prepare the dataset\n",
    "3. Train classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a5aa54-68d1-4afd-8505-e82f942ec1d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1. Get fluxes and clusters for different number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64a16863-dda1-4291-a043-93973e4bbc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_photometry = get_photometry(myFITS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1778f178-cf28-4933-a29f-66b5452a63ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 6\n",
    "classes = []\n",
    "for i in range(num_clusters):\n",
    "    classes.append(i)\n",
    "df_cluster = get_sfh_cluster(myDir,mySFH,num_clusters)\n",
    "df_flux = get_flux_cluster(df_cluster,df_photometry)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302f4fe5-b8eb-4011-8bba-c672cbecdcfb",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2. Prepare the dataset\n",
    "1. Load data into a pandas dataframe\n",
    "2. Convert the dataframe to a numpy array\n",
    "3. Scale the feature dataset\n",
    "4. Use one-hot-encoding for the target variable\n",
    "5. Split into training and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d83af5c-a46e-486c-b9c5-17e9e28e5d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Features before scaling :\n",
      "------------------------------------\n",
      "[[0.28672666 0.32038249 0.50008666 0.96038686 1.64027649]\n",
      " [0.20722867 0.24348071 0.36668962 0.7010275  1.13553877]\n",
      " [0.16796237 0.14784522 0.15487576 0.30383067 0.45300291]\n",
      " [0.36409947 0.385252   0.58739122 1.11396946 1.8537296 ]\n",
      " [0.03021147 0.04522976 0.05992277 0.13502131 0.23168474]]\n",
      "\n",
      "Target before scaling :\n",
      "------------------------------------\n",
      "[1 3 0 2 1]\n",
      "\n",
      "Features after scaling :\n",
      "------------------------------------\n",
      "[[-0.24104421 -0.31081349 -0.32901012 -0.3455134  -0.28738264]\n",
      " [-0.39089942 -0.40103601 -0.39092929 -0.40155184 -0.36778537]\n",
      " [-0.46491714 -0.51323723 -0.48924736 -0.48737211 -0.47651065]\n",
      " [-0.095195   -0.23470744 -0.28848578 -0.3123296  -0.2533804 ]\n",
      " [-0.72458018 -0.63362749 -0.53332188 -0.52384587 -0.51176576]]\n",
      "\n",
      "Target after one-hot-encoding :\n",
      "------------------------------------\n",
      "[[0 1 0 0 0 0]\n",
      " [0 0 0 1 0 0]\n",
      " [1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0]\n",
      " [0 1 0 0 0 0]]\n",
      "\n",
      "Train Test Val Dimensions:\n",
      "------------------------------------\n",
      "(68527, 5) (68527, 6) (11421, 5) (11421, 6) (11422, 5) (11422, 6)\n"
     ]
    }
   ],
   "source": [
    "dataset = df_flux\n",
    "features = dataset[[\"u_FLUX\",\"gHSC_FLUX\",\"rHSC_FLUX\",\"iHSC_FLUX\",\"zHSC_FLUX\"]]\n",
    "labels = dataset[\"cluster\"]\n",
    "X_data = features.to_numpy()\n",
    "Y_data= labels.to_numpy()\n",
    "\n",
    "print(\"\\nFeatures before scaling :\\n------------------------------------\")\n",
    "print(X_data[:5,:])\n",
    "print(\"\\nTarget before scaling :\\n------------------------------------\")\n",
    "print(Y_data[:5])\n",
    "\n",
    "#Create a scaler model that is fit on the input data.\n",
    "scaler = StandardScaler().fit(X_data)\n",
    "\n",
    "#Scale the numeric feature variables\n",
    "X_data = scaler.transform(X_data)\n",
    "\n",
    "#Convert target variable as a one-hot-encoding array\n",
    "Y_data = label_binarize(Y_data,classes=classes[:i+2])\n",
    "\n",
    "print(\"\\nFeatures after scaling :\\n------------------------------------\")\n",
    "print(X_data[:5,:])\n",
    "print(\"\\nTarget after one-hot-encoding :\\n------------------------------------\")\n",
    "print(Y_data[:5])\n",
    "\n",
    "#Split training and test data\n",
    "X_train,X_test,Y_train,Y_test = train_test_split( X_data, Y_data, train_size=0.75)\n",
    "\n",
    "X_test,X_val,Y_test,Y_val = train_test_split( X_test, Y_test, test_size=0.5)\n",
    "\n",
    "print(\"\\nTrain Test Val Dimensions:\\n------------------------------------\")\n",
    "print(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape, X_val.shape, Y_val.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12202354-7c8c-44df-9aa3-1254ae81b391",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 3. Train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3426097-c451-483b-b944-967ae3dc4a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.875317394733429\n"
     ]
    }
   ],
   "source": [
    "classifier = OneVsRestClassifier(LinearSVC(probability=True))\n",
    "#classifier = OneVsRestClassifier(LogisticRegression())\n",
    "classifier.fit(X_train, Y_train)\n",
    "Y_test_hat = classifier.predict(X_test)\n",
    "\n",
    "print('Accuracy:', accuracy_score(Y_test, Y_test_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9589468f-5923-4bc4-827e-6a7dfb923430",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Run pipeline with different cluster numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc5ac599-946a-4229-b47a-bc05d86bbfe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bbc172c342d4b6d966a491099fde21a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7165747284889221\n",
      "0.7734874486923218\n",
      "0.8235706090927124\n",
      "0.8405568599700928\n",
      "0.87724369764328\n",
      "0.8616583347320557\n",
      "0.8920409679412842\n",
      "0.9043866395950317\n",
      "0.9125295281410217\n"
     ]
    }
   ],
   "source": [
    "for num_clusters in tqdm(range(2,11)):\n",
    "    classifier, score = run_pipeline(myDir,mySFH,myFITS,num_clusters)\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb05a8b-1084-4383-9140-fd2810f255e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-23.04",
   "language": "python",
   "name": "module-conda-env-rapids-23.04"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
