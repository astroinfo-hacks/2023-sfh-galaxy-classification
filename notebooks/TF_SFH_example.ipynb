{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Xy2ybWXdCP2s"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"/gpfswork/rech/owt/uwz81lo/2023-sfh-galaxy-classification\")\n",
    "\n",
    "from sfh_classif.dataset import HorizonAGN\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lT57RC0-CjAb"
   },
   "source": [
    "this prepares the input data to the transformer. instead of tokens, you should use the SFR values at each timestep. MAX_TOKENS is the maximum lenght of SFH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "H-nzB9YFCVFH"
   },
   "outputs": [],
   "source": [
    "MAX_TOKENS=128\n",
    "def prepare_batch(sfh):\n",
    "    \"\"\"\n",
    "    Preprocess a batch of Portuguese and English sentences for training a machine translation model.\n",
    "\n",
    "    Args:\n",
    "        pt: A tensor of Portuguese sentences of shape (batch_size,) and dtype tf.string.\n",
    "        en: A tensor of English sentences of shape (batch_size,) and dtype tf.string.\n",
    "\n",
    "    Returns:\n",
    "        A tuple of two tensors representing the input and output sequences for the model, and a tensor of shape\n",
    "        (batch_size, max_length) representing the ground truth output sequences. The input sequence tensor has shape\n",
    "        (batch_size, max_length) and dtype tf.int64, and the output sequence tensor has shape (batch_size, max_length)\n",
    "        and dtype tf.int64.\n",
    "    \"\"\"\n",
    "    #pt = tokenizers.pt.tokenize(pt)      # Output is ragged.\n",
    "    #pt = pt[:, :MAX_TOKENS]    # Trim to MAX_TOKENS.\n",
    "    #pt = pt.to_tensor()  # Convert to 0-padded dense Tensor\n",
    "\n",
    "    #en = tokenizers.en.tokenize(en). ## replace by custome tokenizer\n",
    "    sfh_tok = sfh_tok[:, :(MAX_TOKENS+1)]\n",
    "    sfh_inputs = sfh_tok[:, :-1].to_tensor()  # Drop the [END] tokens\n",
    "    sfh_labels = sfh_tok[:, 1:].to_tensor()   # Drop the [START] tokens\n",
    "\n",
    "    return sfh_inputs, sfh_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NOPHuzs6DK4o"
   },
   "source": [
    "function to feed the transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "tk_AC7vCC0Pu"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 1000\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "def make_batches(ds):\n",
    "  \"\"\"\n",
    "  This function takes a TensorFlow dataset 'ds' and processes it into batches that are ready to be fed to the model.\n",
    "\n",
    "  Parameters:\n",
    "  ds (tf.data.Dataset): TensorFlow dataset to be processed into batches\n",
    "\n",
    "  Returns:\n",
    "  tf.data.Dataset: Processed and batched TensorFlow dataset\n",
    "\n",
    "  \"\"\"\n",
    "  return (\n",
    "      ds.shuffle(BUFFER_SIZE)\n",
    "      .batch(BATCH_SIZE)\n",
    "      .map(prepare_batch, tf.data.AUTOTUNE)\n",
    "      .prefetch(buffer_size=tf.data.AUTOTUNE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset element_spec={'cumulative_mass': TensorSpec(shape=(1220,), dtype=tf.float32, name=None), 'filename': TensorSpec(shape=(), dtype=tf.string, name=None), 'mass': TensorSpec(shape=(1220,), dtype=tf.float32, name=None), 'max_time': TensorSpec(shape=(), dtype=tf.int32, name=None), 'time': TensorSpec(shape=(1220,), dtype=tf.float32, name=None)}>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Ensure tfds does not try to fetch the dataset from the internet\n",
    "os.environ['NO_GCE_CHECK'] = 'true'\n",
    "tfds.core.utils.gcs_utils._is_gcs_disabled = True\n",
    "\n",
    "_JZ_TF_DATASET_DIR = \"/gpfsscratch/rech/owt/commun/galaxy_classification/SFH_tdfs\"\n",
    "\n",
    "# The following line will either load the TensorFlow dataset if existing\n",
    "# or it will create the dataset from the _generate_examples method\n",
    "dset = tfds.load(\"HorizonAGN\", split=tfds.Split.TRAIN, data_dir=_JZ_TF_DATASET_DIR)\n",
    "print(dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "uBkVPT_xDNC3"
   },
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "in user code:\n\n    File \"/tmp/ipykernel_1137217/1139257077.py\", line 21, in prepare_batch  *\n        sfh_tok = sfh_tok[:, :(MAX_TOKENS+1)]\n\n    UnboundLocalError: local variable 'sfh_tok' referenced before assignment\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create training and validation set batches.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_batches \u001b[38;5;241m=\u001b[39m \u001b[43mmake_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sfh_inputs, sfh_labels \u001b[38;5;129;01min\u001b[39;00m train_batches\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      5\u001b[0m   \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36mmake_batches\u001b[0;34m(ds)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_batches\u001b[39m(ds):\n\u001b[1;32m      5\u001b[0m   \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m  This function takes a TensorFlow dataset 'ds' and processes it into batches that are ready to be fed to the model.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m     15\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m     16\u001b[0m       \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBUFFER_SIZE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m---> 18\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprepare_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAUTOTUNE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m       \u001b[38;5;241m.\u001b[39mprefetch(buffer_size\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mAUTOTUNE))\n",
      "File \u001b[0;32m/gpfslocalsup/pub/anaconda-py3/2021.05/envs/tensorflow-2.9.1+py3.10/lib/python3.10/site-packages/tensorflow/python/data/ops/dataset_ops.py:2050\u001b[0m, in \u001b[0;36mDatasetV2.map\u001b[0;34m(self, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m   2048\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m MapDataset(\u001b[38;5;28mself\u001b[39m, map_func, preserve_cardinality\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[1;32m   2049\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2050\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mParallelMapDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2051\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2052\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_parallel_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m      \u001b[49m\u001b[43mpreserve_cardinality\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2056\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/gpfslocalsup/pub/anaconda-py3/2021.05/envs/tensorflow-2.9.1+py3.10/lib/python3.10/site-packages/tensorflow/python/data/ops/dataset_ops.py:5284\u001b[0m, in \u001b[0;36mParallelMapDataset.__init__\u001b[0;34m(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[1;32m   5282\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_dataset \u001b[38;5;241m=\u001b[39m input_dataset\n\u001b[1;32m   5283\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_inter_op_parallelism \u001b[38;5;241m=\u001b[39m use_inter_op_parallelism\n\u001b[0;32m-> 5284\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_func \u001b[38;5;241m=\u001b[39m \u001b[43mstructured_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStructuredFunctionWrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5285\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5286\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transformation_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5287\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5288\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_legacy_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_legacy_function\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5289\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deterministic \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5290\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deterministic \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/gpfslocalsup/pub/anaconda-py3/2021.05/envs/tensorflow-2.9.1+py3.10/lib/python3.10/site-packages/tensorflow/python/data/ops/structured_function.py:271\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m       warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    265\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEven though the `tf.config.experimental_run_functions_eagerly` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    266\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moption is set, this option does not apply to tf.data functions. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    267\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo force eager execution of tf.data functions, please use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    268\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.data.experimental.enable_debug_mode()`.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    269\u001b[0m     fn_factory \u001b[38;5;241m=\u001b[39m trace_tf_function(defun_kwargs)\n\u001b[0;32m--> 271\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function \u001b[38;5;241m=\u001b[39m \u001b[43mfn_factory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;66;03m# There is no graph to add in eager mode.\u001b[39;00m\n\u001b[1;32m    273\u001b[0m add_to_graph \u001b[38;5;241m&\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly()\n",
      "File \u001b[0;32m/gpfslocalsup/pub/anaconda-py3/2021.05/envs/tensorflow-2.9.1+py3.10/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2567\u001b[0m, in \u001b[0;36mFunction.get_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2558\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_concrete_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   2559\u001b[0m   \u001b[38;5;124;03m\"\"\"Returns a `ConcreteFunction` specialized to inputs and execution context.\u001b[39;00m\n\u001b[1;32m   2560\u001b[0m \n\u001b[1;32m   2561\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2565\u001b[0m \u001b[38;5;124;03m       or `tf.Tensor` or `tf.TensorSpec`.\u001b[39;00m\n\u001b[1;32m   2566\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2567\u001b[0m   graph_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_concrete_function_garbage_collected\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2568\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2569\u001b[0m   graph_function\u001b[38;5;241m.\u001b[39m_garbage_collector\u001b[38;5;241m.\u001b[39mrelease()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   2570\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m graph_function\n",
      "File \u001b[0;32m/gpfslocalsup/pub/anaconda-py3/2021.05/envs/tensorflow-2.9.1+py3.10/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2533\u001b[0m, in \u001b[0;36mFunction._get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2531\u001b[0m   args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2532\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m-> 2533\u001b[0m   graph_function, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_define_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2534\u001b[0m   seen_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m   2535\u001b[0m   captured \u001b[38;5;241m=\u001b[39m object_identity\u001b[38;5;241m.\u001b[39mObjectIdentitySet(\n\u001b[1;32m   2536\u001b[0m       graph_function\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39minternal_captures)\n",
      "File \u001b[0;32m/gpfslocalsup/pub/anaconda-py3/2021.05/envs/tensorflow-2.9.1+py3.10/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2711\u001b[0m, in \u001b[0;36mFunction._maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2708\u001b[0m   cache_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_cache\u001b[38;5;241m.\u001b[39mgeneralize(cache_key)\n\u001b[1;32m   2709\u001b[0m   (args, kwargs) \u001b[38;5;241m=\u001b[39m cache_key\u001b[38;5;241m.\u001b[39m_placeholder_value()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m-> 2711\u001b[0m graph_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_graph_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2712\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_cache\u001b[38;5;241m.\u001b[39madd(cache_key, cache_key_deletion_observer,\n\u001b[1;32m   2713\u001b[0m                          graph_function)\n\u001b[1;32m   2715\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m graph_function, filtered_flat_args\n",
      "File \u001b[0;32m/gpfslocalsup/pub/anaconda-py3/2021.05/envs/tensorflow-2.9.1+py3.10/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2627\u001b[0m, in \u001b[0;36mFunction._create_graph_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2622\u001b[0m missing_arg_names \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   2623\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (arg, i) \u001b[38;5;28;01mfor\u001b[39;00m i, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(missing_arg_names)\n\u001b[1;32m   2624\u001b[0m ]\n\u001b[1;32m   2625\u001b[0m arg_names \u001b[38;5;241m=\u001b[39m base_arg_names \u001b[38;5;241m+\u001b[39m missing_arg_names\n\u001b[1;32m   2626\u001b[0m graph_function \u001b[38;5;241m=\u001b[39m ConcreteFunction(\n\u001b[0;32m-> 2627\u001b[0m     \u001b[43mfunc_graph_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc_graph_from_py_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2628\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2629\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_python_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2630\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2631\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2632\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_signature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2633\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautograph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_autograph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2634\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautograph_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_autograph_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2635\u001b[0m \u001b[43m        \u001b[49m\u001b[43marg_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marg_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2636\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapture_by_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_capture_by_value\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   2637\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_attributes,\n\u001b[1;32m   2638\u001b[0m     spec\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_spec,\n\u001b[1;32m   2639\u001b[0m     \u001b[38;5;66;03m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[39;00m\n\u001b[1;32m   2640\u001b[0m     \u001b[38;5;66;03m# scope. This is not the default behavior since it gets used in some\u001b[39;00m\n\u001b[1;32m   2641\u001b[0m     \u001b[38;5;66;03m# places (like Keras) where the FuncGraph lives longer than the\u001b[39;00m\n\u001b[1;32m   2642\u001b[0m     \u001b[38;5;66;03m# ConcreteFunction.\u001b[39;00m\n\u001b[1;32m   2643\u001b[0m     shared_func_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   2644\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m graph_function\n",
      "File \u001b[0;32m/gpfslocalsup/pub/anaconda-py3/2021.05/envs/tensorflow-2.9.1+py3.10/lib/python3.10/site-packages/tensorflow/python/framework/func_graph.py:1141\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1139\u001b[0m   _, original_func \u001b[38;5;241m=\u001b[39m tf_decorator\u001b[38;5;241m.\u001b[39munwrap(python_func)\n\u001b[0;32m-> 1141\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mpython_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunc_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunc_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[38;5;66;03m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[1;32m   1144\u001b[0m \u001b[38;5;66;03m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[1;32m   1145\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m nest\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[1;32m   1146\u001b[0m     convert, func_outputs, expand_composites\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/gpfslocalsup/pub/anaconda-py3/2021.05/envs/tensorflow-2.9.1+py3.10/lib/python3.10/site-packages/tensorflow/python/data/ops/structured_function.py:248\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__.<locals>.trace_tf_function.<locals>.wrapped_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;129m@eager_function\u001b[39m\u001b[38;5;241m.\u001b[39mdefun_with_attributes(\n\u001b[1;32m    243\u001b[0m     input_signature\u001b[38;5;241m=\u001b[39mstructure\u001b[38;5;241m.\u001b[39mget_flat_tensor_specs(\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_structure),\n\u001b[1;32m    245\u001b[0m     autograph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    246\u001b[0m     attributes\u001b[38;5;241m=\u001b[39mdefun_kwargs)\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_fn\u001b[39m(\u001b[38;5;241m*\u001b[39margs):  \u001b[38;5;66;03m# pylint: disable=missing-docstring\u001b[39;00m\n\u001b[0;32m--> 248\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mwrapper_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m   ret \u001b[38;5;241m=\u001b[39m structure\u001b[38;5;241m.\u001b[39mto_tensor_list(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_structure, ret)\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m [ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m ret]\n",
      "File \u001b[0;32m/gpfslocalsup/pub/anaconda-py3/2021.05/envs/tensorflow-2.9.1+py3.10/lib/python3.10/site-packages/tensorflow/python/data/ops/structured_function.py:177\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__.<locals>.wrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _should_unpack(nested_args):\n\u001b[1;32m    176\u001b[0m   nested_args \u001b[38;5;241m=\u001b[39m (nested_args,)\n\u001b[0;32m--> 177\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43mautograph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtf_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag_ctx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnested_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _should_pack(ret):\n\u001b[1;32m    179\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(ret)\n",
      "File \u001b[0;32m/gpfslocalsup/pub/anaconda-py3/2021.05/envs/tensorflow-2.9.1+py3.10/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:692\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m    691\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 692\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mag_error_metadata\u001b[38;5;241m.\u001b[39mto_exception(e)\n\u001b[1;32m    693\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    694\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m/gpfslocalsup/pub/anaconda-py3/2021.05/envs/tensorflow-2.9.1+py3.10/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:689\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    688\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m conversion_ctx:\n\u001b[0;32m--> 689\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m    691\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[0;32m/gpfslocalsup/pub/anaconda-py3/2021.05/envs/tensorflow-2.9.1+py3.10/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:439\u001b[0m, in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    438\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 439\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mconverted_f\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43meffective_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    441\u001b[0m     result \u001b[38;5;241m=\u001b[39m converted_f(\u001b[38;5;241m*\u001b[39meffective_args)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filetkyhu7ew.py:23\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__prepare_batch\u001b[0;34m(sfh)\u001b[0m\n\u001b[1;32m     21\u001b[0m do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     22\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[0;32m---> 23\u001b[0m sfh_tok \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(\u001b[43msfh_tok\u001b[49m)[:, :ag__\u001b[38;5;241m.\u001b[39mld(MAX_TOKENS) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     24\u001b[0m sfh_inputs \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(sfh_tok)[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mto_tensor, (), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     25\u001b[0m sfh_labels \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(sfh_tok)[:, \u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mto_tensor, (), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: in user code:\n\n    File \"/tmp/ipykernel_1137217/1139257077.py\", line 21, in prepare_batch  *\n        sfh_tok = sfh_tok[:, :(MAX_TOKENS+1)]\n\n    UnboundLocalError: local variable 'sfh_tok' referenced before assignment\n"
     ]
    }
   ],
   "source": [
    "# Create training and validation set batches.\n",
    "train_batches = make_batches(dset)\n",
    "\n",
    "for sfh_inputs, sfh_labels in train_batches.take(1):\n",
    "  break\n",
    "\n",
    "print(f'sfh_labels.shape: {sfh_labels.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_r6a_ziEDfbM"
   },
   "source": [
    "positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y3dDRpk6DZF-"
   },
   "outputs": [],
   "source": [
    "def positional_encoding(length, depth):\n",
    "  \"\"\"\n",
    "  Generates a matrix of position encodings for an input sequence.\n",
    "\n",
    "  Args:\n",
    "      length: An integer representing the length of the input sequence.\n",
    "      depth: An integer representing the dimensionality of the encoding.\n",
    "\n",
    "  Returns:\n",
    "      A `tf.Tensor` of shape `(length, depth)` representing the position encoding matrix.\n",
    "  \"\"\"\n",
    "  depth = depth/2\n",
    "\n",
    "  positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
    "  depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
    "\n",
    "  angle_rates = 1 / (10000**depths)         # (1, depth)\n",
    "  angle_rads = positions * angle_rates      # (pos, depth)\n",
    "\n",
    "  pos_encoding = np.concatenate(\n",
    "      [np.sin(angle_rads), np.cos(angle_rads)],\n",
    "      axis=-1)\n",
    "\n",
    "  return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uPjm34LdDjin"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "pos_encoding = positional_encoding(length=2048, depth=512)\n",
    "\n",
    "# Check the shape.\n",
    "print(pos_encoding.shape)\n",
    "\n",
    "# Plot the dimensions.\n",
    "plt.pcolormesh(pos_encoding.numpy().T, cmap='RdBu')\n",
    "plt.ylabel('Depth')\n",
    "plt.xlabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ion2NVtVGQaV"
   },
   "outputs": [],
   "source": [
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "  \"\"\"\n",
    "  This layer combines the input embedding with a positional encoding that helps the Transformer to understand\n",
    "  the relative position of the tokens in a sequence. It takes an input sequence of tokens and converts it to\n",
    "  a sequence of embedding vectors, then adds positional information to it.\n",
    "\n",
    "  Attributes:\n",
    "      vocab_size (int): The size of the vocabulary, i.e., the number of unique tokens in the input sequence.\n",
    "      d_model (int): The number of dimensions in the embedding vector.\n",
    "\n",
    "  Methods:\n",
    "      compute_mask(*args, **kwargs): This method computes the mask to be applied to the embeddings.\n",
    "      call(x): This method performs the computation for the layer.\n",
    "\n",
    "  \"\"\"\n",
    "  def __init__(self, vocab_size, d_model):\n",
    "    \"\"\"\n",
    "    Initializes the PositionalEmbedding layer.\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int): The size of the vocabulary, i.e., the number of unique tokens in the input sequence.\n",
    "        d_model (int): The number of dimensions in the embedding vector.\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    self.d_model = d_model\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)\n",
    "    self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n",
    "\n",
    "  def compute_mask(self, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Computes the mask to be applied to the embeddings.\n",
    "\n",
    "    Args:\n",
    "        *args: Variable length argument list.\n",
    "        **kwargs: Arbitrary keyword arguments.\n",
    "\n",
    "    Returns:\n",
    "        Mask to be applied to the embeddings.\n",
    "    \"\"\"\n",
    "    return self.embedding.compute_mask(*args, **kwargs)\n",
    "\n",
    "  def call(self, x):\n",
    "    \"\"\"\n",
    "    Computes the output of the layer.\n",
    "\n",
    "    Args:\n",
    "        x (tf.Tensor): Input sequence of tokens.\n",
    "\n",
    "    Returns:\n",
    "        The output sequence of embedding vectors with added positional information.\n",
    "    \"\"\"\n",
    "    length = tf.shape(x)[1]\n",
    "    x = self.embedding(x)\n",
    "    # This factor sets the relative scale of the embedding and positonal_encoding.\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fqFGukQkGYf1"
   },
   "outputs": [],
   "source": [
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "  \"\"\"\n",
    "  Base Attention layer class that contains a MultiHeadAttention, LayerNormalization and Add layer.\n",
    "\n",
    "  Attributes:\n",
    "  -----------\n",
    "  kwargs: dict\n",
    "      keyword arguments that will be passed to the MultiHeadAttention layer during initialization.\n",
    "\n",
    "  Methods:\n",
    "  --------\n",
    "  call(inputs, mask=None, training=None):\n",
    "      Performs a forward pass on the input and returns the output.\n",
    "\n",
    "  \"\"\"\n",
    "  def __init__(self, **kwargs):\n",
    "    \"\"\"\n",
    "    Initializes a new instance of the BaseAttention layer class.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    kwargs: dict\n",
    "        keyword arguments that will be passed to the MultiHeadAttention layer during initialization.\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "    self.add = tf.keras.layers.Add()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ENI92FCLGw5A"
   },
   "outputs": [],
   "source": [
    "class GlobalSelfAttention(BaseAttention):\n",
    "  def call(self, x):\n",
    "    \"\"\"\n",
    "    Apply the global self-attention mechanism to the input sequence.\n",
    "\n",
    "    Args:\n",
    "        x: A tensor of shape `(batch_size, seq_len, embedding_dim)`\n",
    "        representing the input sequence.\n",
    "\n",
    "    Returns:\n",
    "        A tensor of the same shape as the input, representing the sequence\n",
    "        after being transformed by the self-attention mechanism.\n",
    "    \"\"\"\n",
    "    attn_output = self.mha(\n",
    "        query=x,\n",
    "        value=x,\n",
    "        key=x)\n",
    "    x = self.add([x, attn_output])\n",
    "    x = self.layernorm(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YsrO4tE4HA5Y"
   },
   "outputs": [],
   "source": [
    "class CausalSelfAttention(BaseAttention):\n",
    "  \"\"\"\n",
    "  Call self attention on the input sequence, ensuring that each position in the\n",
    "  output depends only on previous positions (i.e. a causal model).\n",
    "\n",
    "  Args:\n",
    "      x: Input sequence tensor of shape `(batch_size, seq_len, embed_dim)`.\n",
    "\n",
    "  Returns:\n",
    "      Output sequence tensor of the same shape as the input, after self-attention\n",
    "      and residual connection with layer normalization applied.\n",
    "  \"\"\"\n",
    "  def call(self, x):\n",
    "    attn_output = self.mha(\n",
    "        query=x,\n",
    "        value=x,\n",
    "        key=x,\n",
    "        use_causal_mask = True)\n",
    "    x = self.add([x, attn_output])\n",
    "    x = self.layernorm(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58SORW4HHapL"
   },
   "source": [
    "checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9vAsstcOHT-9"
   },
   "outputs": [],
   "source": [
    "sample_csa = CausalSelfAttention(num_heads=2, key_dim=512)\n",
    "\n",
    "print(en_emb.shape)\n",
    "print(sample_csa(en_emb).shape)\n",
    "\n",
    "out1 = sample_csa(embed_en(en[:, :3]))\n",
    "out2 = sample_csa(embed_en(en))[:, :3]\n",
    "\n",
    "tf.reduce_max(abs(out1 - out2)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LCZ_3CmiHcdf"
   },
   "outputs": [],
   "source": [
    "class FeedForward(tf.keras.layers.Layer):\n",
    "  \"\"\"\n",
    "  Implements the feedforward sublayer of the transformer block.\n",
    "\n",
    "  Parameters:\n",
    "  -----------\n",
    "  d_model: int\n",
    "      The number of expected features in the input and output.\n",
    "  dff: int\n",
    "      The number of neurons in the first Dense layer.\n",
    "  dropout_rate: float, optional (default=0.1)\n",
    "      The dropout rate to use.\n",
    "\n",
    "  Attributes:\n",
    "  -----------\n",
    "  seq: tf.keras.Sequential\n",
    "      The sequential model that applies the two Dense layers and Dropout.\n",
    "  add: tf.keras.layers.Add\n",
    "      The addition layer that adds the residual connection.\n",
    "  layer_norm: tf.keras.layers.LayerNormalization\n",
    "      The normalization layer applied to the output.\n",
    "\n",
    "  Methods:\n",
    "  --------\n",
    "  call(x):\n",
    "      Computes the feedforward sublayer on the input tensor x and returns the output.\n",
    "\n",
    "  \"\"\"\n",
    "  def __init__(self, d_model, dff, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "    self.seq = tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),\n",
    "      tf.keras.layers.Dense(d_model),\n",
    "      tf.keras.layers.Dropout(dropout_rate)\n",
    "    ])\n",
    "    self.add = tf.keras.layers.Add()\n",
    "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "  def call(self, x):\n",
    "    \"\"\"\n",
    "    Passes the input tensor `x` through a feedforward network consisting of two\n",
    "    dense layers with `dff` hidden units and a `relu` activation function.\n",
    "    A `dropout_rate` is applied after the first dense layer to prevent overfitting.\n",
    "    The output of the feedforward network is added to the original input `x` via the\n",
    "    `Add()` layer. Finally, the output is normalized using the `LayerNormalization()` layer.\n",
    "\n",
    "    Args:\n",
    "        x (tf.Tensor): Input tensor with shape `(batch_size, seq_len, d_model)`.\n",
    "\n",
    "    Returns:\n",
    "        tf.Tensor: Output tensor with shape `(batch_size, seq_len, d_model)`.\n",
    "    \"\"\"\n",
    "    x = self.add([x, self.seq(x)])\n",
    "    x = self.layer_norm(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GFlWA0-XHnEM"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "  \"\"\"\n",
    "  A single layer of the decoder in a transformer-based architecture.\n",
    "\n",
    "  Args:\n",
    "    d_model (int): The number of expected features in the input.\n",
    "    num_heads (int): The number of attention heads.\n",
    "    dff (int): The dimensionality of the feedforward network.\n",
    "    dropout_rate (float): The dropout rate to be applied.\n",
    "\n",
    "  Attributes:\n",
    "    causal_self_attention: An instance of the `CausalSelfAttention` layer.\n",
    "    cross_attention: An instance of the `CrossAttention` layer.\n",
    "    ffn: An instance of the `FeedForward` layer.\n",
    "    last_attn_scores: A tensor containing the last attention scores.\n",
    "\n",
    "  \"\"\"\n",
    "  def __init__(self,\n",
    "               *,\n",
    "               d_model,\n",
    "               num_heads,\n",
    "               dff,\n",
    "               dropout_rate=0.1):\n",
    "    super(DecoderLayer, self).__init__()\n",
    "\n",
    "    self.causal_self_attention = CausalSelfAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=d_model,\n",
    "        dropout=dropout_rate)\n",
    "\n",
    "    self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "  def call(self, x, context):\n",
    "    \"\"\"\n",
    "    Forward pass of the `DecoderLayer`.\n",
    "\n",
    "    Args:\n",
    "      x (tf.Tensor): The input tensor of shape\n",
    "      `(batch_size, target_seq_len, d_model)`.\n",
    "      context (tf.Tensor): The context tensor of shape\n",
    "      `(batch_size, input_seq_len, d_model)`.\n",
    "\n",
    "    Returns:\n",
    "      The output tensor of the `DecoderLayer` of shape\n",
    "      `(batch_size, target_seq_len, d_model)`.\n",
    "\n",
    "    \"\"\"\n",
    "    x = self.causal_self_attention(x=x)\n",
    "\n",
    "    # Cache the last attention scores for plotting later\n",
    "    self.last_attn_scores = self.cross_attention.last_attn_scores\n",
    "\n",
    "    x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L9j5QyPNH4Me"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "  \"\"\"A decoder model for sequence to sequence learning.\n",
    "\n",
    "  This class implements a decoder layer for a transformer-based model used for sequence to sequence learning tasks. The decoder layer takes input embeddings, positional encodings, and attention masks as input, and returns the output of the decoder layer after applying a multi-head self-attention mechanism, followed by a cross-attention mechanism with the output from the encoder layers, and then applying a feed-forward neural network.\n",
    "\n",
    "  Attributes:\n",
    "    d_model (int): The number of output dimensions for each layer.\n",
    "    num_layers (int): The number of layers in the decoder.\n",
    "    pos_embedding (PositionalEmbedding): The positional embedding layer.\n",
    "    dropout (Dropout): A dropout layer.\n",
    "    dec_layers (list): A list of DecoderLayer objects.\n",
    "    last_attn_scores (ndarray): The attention scores from the last decoder layer.\n",
    "\n",
    "  Methods:\n",
    "    call(x, context): Implements the forward pass for the decoder layer.\n",
    "      Args:\n",
    "        x (ndarray): A tensor of shape (batch_size, target_seq_len), representing the input token IDs.\n",
    "        context (ndarray): A tensor of shape (batch_size, input_seq_len, d_model), representing the output from the encoder layers.\n",
    "      Returns:\n",
    "        ndarray: A tensor of shape (batch_size, target_seq_len, d_model), representing the output from the decoder layers.\n",
    "  \"\"\"\n",
    "  def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size,\n",
    "               dropout_rate=0.1):\n",
    "    super(Decoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size,\n",
    "                                             d_model=d_model)\n",
    "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "    self.dec_layers = [\n",
    "        DecoderLayer(d_model=d_model, num_heads=num_heads,\n",
    "                     dff=dff, dropout_rate=dropout_rate)\n",
    "        for _ in range(num_layers)]\n",
    "\n",
    "    self.last_attn_scores = None\n",
    "\n",
    "  def call(self, x, context):\n",
    "    \"\"\"\n",
    "    Implements the forward pass for the decoder layer.\n",
    "\n",
    "    Args:\n",
    "      x (ndarray): A tensor of shape (batch_size, target_seq_len), representing the input token IDs.\n",
    "      context (ndarray): A tensor of shape (batch_size, input_seq_len, d_model), representing the output from the encoder layers.\n",
    "\n",
    "    Returns:\n",
    "      ndarray: A tensor of shape (batch_size, target_seq_len, d_model), representing the output from the decoder layers.\n",
    "    \"\"\"\n",
    "    # `x` is token-IDs shape (batch, target_seq_len)\n",
    "    x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "    x = self.dropout(x)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      x  = self.dec_layers[i](x, context)\n",
    "\n",
    "    self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
    "\n",
    "    # The shape of x is (batch_size, target_seq_len, d_model).\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DWzsFqNgIMWp"
   },
   "outputs": [],
   "source": [
    "# Instantiate the decoder.\n",
    "sample_decoder = Decoder(num_layers=4,\n",
    "                         d_model=512,\n",
    "                         num_heads=8,\n",
    "                         dff=2048,\n",
    "                         vocab_size=8000)\n",
    "\n",
    "output = sample_decoder(\n",
    "    x=en,\n",
    "    context=pt_emb)\n",
    "\n",
    "# Print the shapes.\n",
    "print(en.shape)\n",
    "print(pt_emb.shape)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "flmHgGz8IUFW"
   },
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "  \"\"\"\n",
    "  A transformer model that consists of an encoder, a decoder and a final dense layer.\n",
    "\n",
    "  Args:\n",
    "    num_layers (int): Number of layers in both the encoder and decoder.\n",
    "    d_model (int): Hidden size of the model.\n",
    "    num_heads (int): Number of attention heads used in the model.\n",
    "    dff (int): Size of the feedforward layer in the encoder and decoder.\n",
    "    input_vocab_size (int): Size of the vocabulary of the input.\n",
    "    target_vocab_size (int): Size of the vocabulary of the target.\n",
    "    dropout_rate (float): Dropout rate applied to the output of each sub-layer.\n",
    "\n",
    "  Attributes:\n",
    "    encoder (Encoder): An instance of the Encoder class.\n",
    "    decoder (Decoder): An instance of the Decoder class.\n",
    "    final_layer (Dense): A Dense layer that converts the final transformer output to output token probabilities.\n",
    "\n",
    "  Methods:\n",
    "    call(inputs): Forward pass of the transformer model.\n",
    "\n",
    "  Returns:\n",
    "    logits (tf.Tensor): Output tensor of the final dense layer. Shape (batch_size, target_len, target_vocab_size).\n",
    "  \"\"\"\n",
    "  def __init__(self, *, num_layers, d_model, num_heads, dff,\n",
    "               input_vocab_size, target_vocab_size, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "\n",
    "    self.decoder = Decoder(num_layers=num_layers, d_model=d_model,\n",
    "                           num_heads=num_heads, dff=dff,\n",
    "                           vocab_size=target_vocab_size,\n",
    "                           dropout_rate=dropout_rate)\n",
    "\n",
    "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    \"\"\"\n",
    "    Forward pass of the transformer model.\n",
    "\n",
    "    Args:\n",
    "      inputs (tuple): A tuple of two tensors. The first tensor is the context input tensor of shape (batch_size, context_len).\n",
    "                      The second tensor is the target input tensor of shape (batch_size, target_len).\n",
    "\n",
    "    Returns:\n",
    "      logits (tf.Tensor): Output tensor of the final dense layer. Shape (batch_size, target_len, target_vocab_size).\n",
    "    \"\"\"\n",
    "\n",
    "    # To use a Keras model with `.fit` you must pass all your inputs in the\n",
    "    # first argument.\n",
    "    x  = inputs\n",
    "\n",
    "    #context = self.encoder(context)  # (batch_size, context_len, d_model)\n",
    "\n",
    "    x = self.decoder(x)  # (batch_size, target_len, d_model)\n",
    "\n",
    "    # Final linear layer output.\n",
    "    logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)\n",
    "\n",
    "    try:\n",
    "      # Drop the keras mask, so it doesn't scale the losses/metrics.\n",
    "      # b/250038731\n",
    "      del logits._keras_mask\n",
    "    except AttributeError:\n",
    "      pass\n",
    "\n",
    "    # Return the final output and the attention weights.\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gImmN6hEIgo9"
   },
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N-ha2MJmIjxn"
   },
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    dff=dff,\n",
    "    input_vocab_size=tokenizers.pt.get_vocab_size().numpy(),\n",
    "    target_vocab_size=tokenizers.en.get_vocab_size().numpy(),\n",
    "    dropout_rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HO3PCHcHInXP"
   },
   "outputs": [],
   "source": [
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZBytL287IsRl"
   },
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  \"\"\"\n",
    "  Custom learning rate schedule that implements the learning rate function\n",
    "  described in the original Transformer paper. The learning rate is increased\n",
    "  linearly for the first `warmup_steps` training steps, and then decreased\n",
    "  proportionally to the inverse square root of the step number.\n",
    "\n",
    "  Args:\n",
    "    d_model (int): the dimensionality of the model.\n",
    "    warmup_steps (int): the number of steps taken to increase the learning rate\n",
    "      linearly. Default is 4000.\n",
    "\n",
    "  Attributes:\n",
    "    d_model (float): the dimensionality of the model as a float.\n",
    "    warmup_steps (int): the number of steps taken to increase the learning rate\n",
    "      linearly.\n",
    "\n",
    "  Methods:\n",
    "    __call__(step): returns the learning rate at the given step.\n",
    "\n",
    "  Returns:\n",
    "    The learning rate at the given step.\n",
    "  \"\"\"\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super().__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    \"\"\"\n",
    "    Returns the learning rate at the given step.\n",
    "\n",
    "    Args:\n",
    "      step (int): the current training step.\n",
    "\n",
    "    Returns:\n",
    "      The learning rate at the given step as a float32 tensor.\n",
    "    \"\"\"\n",
    "    step = tf.cast(step, dtype=tf.float32)\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BlicjSDzIsv5"
   },
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
    "                                     epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oeCXNP-hIvC8"
   },
   "outputs": [],
   "source": [
    "def masked_loss(label, pred):\n",
    "  \"\"\"\n",
    "  Calculates the masked sparse categorical cross-entropy loss between the true labels and predicted labels.\n",
    "\n",
    "  Args:\n",
    "      label: A tensor of shape (batch_size, seq_length) containing the true labels.\n",
    "      pred: A tensor of shape (batch_size, seq_length, target_vocab_size) containing the predicted labels.\n",
    "\n",
    "  Returns:\n",
    "      A scalar tensor representing the masked loss value.\n",
    "\n",
    "  \"\"\"\n",
    "  mask = label != 0\n",
    "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "  loss = loss_object(label, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss.dtype)\n",
    "  loss *= mask\n",
    "\n",
    "  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VfrjqxP7I-5q"
   },
   "outputs": [],
   "source": [
    "def masked_accuracy(label, pred):\n",
    "  \"\"\"\n",
    "  Calculates the masked accuracy between the true labels and predicted labels.\n",
    "\n",
    "  Args:\n",
    "      label: A tensor of shape (batch_size, seq_length) containing the true labels.\n",
    "      pred: A tensor of shape (batch_size, seq_length, target_vocab_size) containing the predicted labels.\n",
    "\n",
    "  Returns:\n",
    "      A scalar tensor representing the masked accuracy value.\n",
    "\n",
    "  \"\"\"\n",
    "  pred = tf.argmax(pred, axis=2)\n",
    "  label = tf.cast(label, pred.dtype)\n",
    "  match = label == pred\n",
    "\n",
    "  mask = label != 0\n",
    "\n",
    "  match = match & mask\n",
    "\n",
    "  match = tf.cast(match, dtype=tf.float32)\n",
    "  mask = tf.cast(mask, dtype=tf.float32)\n",
    "  return tf.reduce_sum(match)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UMkzTRd9I_dP"
   },
   "outputs": [],
   "source": [
    "transformer.compile(\n",
    "    loss=masked_loss,\n",
    "    optimizer=optimizer,\n",
    "    metrics=[masked_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nZHdmO1EJBhu"
   },
   "outputs": [],
   "source": [
    "transformer.fit(train_batches,\n",
    "                epochs=20,\n",
    "                validation_data=val_batches)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tensorflow-2.9.1_py3.10",
   "language": "python",
   "name": "module-conda-env-tensorflow-2.9.1_py3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}