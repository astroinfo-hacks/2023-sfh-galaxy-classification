{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xy2ybWXdCP2s"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "this prepares the input data to the transformer. instead of tokens, you should use the SFR values at each timestep. MAX_TOKENS is the maximum lenght of SFH."
      ],
      "metadata": {
        "id": "lT57RC0-CjAb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_TOKENS=128\n",
        "def prepare_batch(sfh):\n",
        "    \"\"\"\n",
        "    Preprocess a batch of Portuguese and English sentences for training a machine translation model.\n",
        "\n",
        "    Args:\n",
        "        pt: A tensor of Portuguese sentences of shape (batch_size,) and dtype tf.string.\n",
        "        en: A tensor of English sentences of shape (batch_size,) and dtype tf.string.\n",
        "\n",
        "    Returns:\n",
        "        A tuple of two tensors representing the input and output sequences for the model, and a tensor of shape\n",
        "        (batch_size, max_length) representing the ground truth output sequences. The input sequence tensor has shape\n",
        "        (batch_size, max_length) and dtype tf.int64, and the output sequence tensor has shape (batch_size, max_length)\n",
        "        and dtype tf.int64.\n",
        "    \"\"\"\n",
        "    #pt = tokenizers.pt.tokenize(pt)      # Output is ragged.\n",
        "    #pt = pt[:, :MAX_TOKENS]    # Trim to MAX_TOKENS.\n",
        "    #pt = pt.to_tensor()  # Convert to 0-padded dense Tensor\n",
        "\n",
        "    #en = tokenizers.en.tokenize(en). ## replace by custome tokenizer\n",
        "    sfh_tok = sfh_tok[:, :(MAX_TOKENS+1)]\n",
        "    sfh_inputs = sfh_tok[:, :-1].to_tensor()  # Drop the [END] tokens\n",
        "    sfh_labels = sfh_tok[:, 1:].to_tensor()   # Drop the [START] tokens\n",
        "\n",
        "    return sfh_inputs, sfh_labels"
      ],
      "metadata": {
        "id": "H-nzB9YFCVFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "function to feed the transformer"
      ],
      "metadata": {
        "id": "NOPHuzs6DK4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = 20000\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "def make_batches(ds):\n",
        "  \"\"\"\n",
        "  This function takes a TensorFlow dataset 'ds' and processes it into batches that are ready to be fed to the model.\n",
        "\n",
        "  Parameters:\n",
        "  ds (tf.data.Dataset): TensorFlow dataset to be processed into batches\n",
        "\n",
        "  Returns:\n",
        "  tf.data.Dataset: Processed and batched TensorFlow dataset\n",
        "\n",
        "  \"\"\"\n",
        "  return (\n",
        "      ds\n",
        "      .shuffle(BUFFER_SIZE)\n",
        "      .batch(BATCH_SIZE)\n",
        "      .map(prepare_batch, tf.data.AUTOTUNE)\n",
        "      .prefetch(buffer_size=tf.data.AUTOTUNE))"
      ],
      "metadata": {
        "id": "tk_AC7vCC0Pu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training and validation set batches.\n",
        "train_batches = make_batches(train_examples)\n",
        "val_batches = make_batches(val_examples)\n",
        "\n",
        "for sfh_inputs, sfh_labels in train_batches.take(1):\n",
        "  break\n",
        "\n",
        "\n",
        "print(f'sfh_labels.shape: {sfh_labels.shape}')\n",
        "print(f'sfh_inputs.shape: {sfh_inputs.shape}')"
      ],
      "metadata": {
        "id": "uBkVPT_xDNC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "positional encoding"
      ],
      "metadata": {
        "id": "_r6a_ziEDfbM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def positional_encoding(length, depth):\n",
        "  \"\"\"\n",
        "  Generates a matrix of position encodings for an input sequence.\n",
        "\n",
        "  Args:\n",
        "      length: An integer representing the length of the input sequence.\n",
        "      depth: An integer representing the dimensionality of the encoding.\n",
        "\n",
        "  Returns:\n",
        "      A `tf.Tensor` of shape `(length, depth)` representing the position encoding matrix.\n",
        "  \"\"\"\n",
        "  depth = depth/2\n",
        "\n",
        "  positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
        "  depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
        "\n",
        "  angle_rates = 1 / (10000**depths)         # (1, depth)\n",
        "  angle_rads = positions * angle_rates      # (pos, depth)\n",
        "\n",
        "  pos_encoding = np.concatenate(\n",
        "      [np.sin(angle_rads), np.cos(angle_rads)],\n",
        "      axis=-1)\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "metadata": {
        "id": "Y3dDRpk6DZF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "pos_encoding = positional_encoding(length=2048, depth=512)\n",
        "\n",
        "# Check the shape.\n",
        "print(pos_encoding.shape)\n",
        "\n",
        "# Plot the dimensions.\n",
        "plt.pcolormesh(pos_encoding.numpy().T, cmap='RdBu')\n",
        "plt.ylabel('Depth')\n",
        "plt.xlabel('Position')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uPjm34LdDjin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(tf.keras.layers.Layer):\n",
        "  \"\"\"\n",
        "  This layer combines the input embedding with a positional encoding that helps the Transformer to understand\n",
        "  the relative position of the tokens in a sequence. It takes an input sequence of tokens and converts it to\n",
        "  a sequence of embedding vectors, then adds positional information to it.\n",
        "\n",
        "  Attributes:\n",
        "      vocab_size (int): The size of the vocabulary, i.e., the number of unique tokens in the input sequence.\n",
        "      d_model (int): The number of dimensions in the embedding vector.\n",
        "\n",
        "  Methods:\n",
        "      compute_mask(*args, **kwargs): This method computes the mask to be applied to the embeddings.\n",
        "      call(x): This method performs the computation for the layer.\n",
        "\n",
        "  \"\"\"\n",
        "  def __init__(self, vocab_size, d_model):\n",
        "    \"\"\"\n",
        "    Initializes the PositionalEmbedding layer.\n",
        "\n",
        "    Args:\n",
        "        vocab_size (int): The size of the vocabulary, i.e., the number of unique tokens in the input sequence.\n",
        "        d_model (int): The number of dimensions in the embedding vector.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)\n",
        "    self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n",
        "\n",
        "  def compute_mask(self, *args, **kwargs):\n",
        "    \"\"\"\n",
        "    Computes the mask to be applied to the embeddings.\n",
        "\n",
        "    Args:\n",
        "        *args: Variable length argument list.\n",
        "        **kwargs: Arbitrary keyword arguments.\n",
        "\n",
        "    Returns:\n",
        "        Mask to be applied to the embeddings.\n",
        "    \"\"\"\n",
        "    return self.embedding.compute_mask(*args, **kwargs)\n",
        "\n",
        "  def call(self, x):\n",
        "    \"\"\"\n",
        "    Computes the output of the layer.\n",
        "\n",
        "    Args:\n",
        "        x (tf.Tensor): Input sequence of tokens.\n",
        "\n",
        "    Returns:\n",
        "        The output sequence of embedding vectors with added positional information.\n",
        "    \"\"\"\n",
        "    length = tf.shape(x)[1]\n",
        "    x = self.embedding(x)\n",
        "    # This factor sets the relative scale of the embedding and positonal_encoding.\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "ion2NVtVGQaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseAttention(tf.keras.layers.Layer):\n",
        "  \"\"\"\n",
        "  Base Attention layer class that contains a MultiHeadAttention, LayerNormalization and Add layer.\n",
        "\n",
        "  Attributes:\n",
        "  -----------\n",
        "  kwargs: dict\n",
        "      keyword arguments that will be passed to the MultiHeadAttention layer during initialization.\n",
        "\n",
        "  Methods:\n",
        "  --------\n",
        "  call(inputs, mask=None, training=None):\n",
        "      Performs a forward pass on the input and returns the output.\n",
        "\n",
        "  \"\"\"\n",
        "  def __init__(self, **kwargs):\n",
        "    \"\"\"\n",
        "    Initializes a new instance of the BaseAttention layer class.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    kwargs: dict\n",
        "        keyword arguments that will be passed to the MultiHeadAttention layer during initialization.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "    self.add = tf.keras.layers.Add()"
      ],
      "metadata": {
        "id": "fqFGukQkGYf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GlobalSelfAttention(BaseAttention):\n",
        "  def call(self, x):\n",
        "    \"\"\"\n",
        "    Apply the global self-attention mechanism to the input sequence.\n",
        "\n",
        "    Args:\n",
        "        x: A tensor of shape `(batch_size, seq_len, embedding_dim)`\n",
        "        representing the input sequence.\n",
        "\n",
        "    Returns:\n",
        "        A tensor of the same shape as the input, representing the sequence\n",
        "        after being transformed by the self-attention mechanism.\n",
        "    \"\"\"\n",
        "    attn_output = self.mha(\n",
        "        query=x,\n",
        "        value=x,\n",
        "        key=x)\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "ENI92FCLGw5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalSelfAttention(BaseAttention):\n",
        "  \"\"\"\n",
        "  Call self attention on the input sequence, ensuring that each position in the\n",
        "  output depends only on previous positions (i.e. a causal model).\n",
        "\n",
        "  Args:\n",
        "      x: Input sequence tensor of shape `(batch_size, seq_len, embed_dim)`.\n",
        "\n",
        "  Returns:\n",
        "      Output sequence tensor of the same shape as the input, after self-attention\n",
        "      and residual connection with layer normalization applied.\n",
        "  \"\"\"\n",
        "  def call(self, x):\n",
        "    attn_output = self.mha(\n",
        "        query=x,\n",
        "        value=x,\n",
        "        key=x,\n",
        "        use_causal_mask = True)\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "YsrO4tE4HA5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "checking"
      ],
      "metadata": {
        "id": "58SORW4HHapL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_csa = CausalSelfAttention(num_heads=2, key_dim=512)\n",
        "\n",
        "print(en_emb.shape)\n",
        "print(sample_csa(en_emb).shape)\n",
        "\n",
        "out1 = sample_csa(embed_en(en[:, :3]))\n",
        "out2 = sample_csa(embed_en(en))[:, :3]\n",
        "\n",
        "tf.reduce_max(abs(out1 - out2)).numpy()"
      ],
      "metadata": {
        "id": "9vAsstcOHT-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(tf.keras.layers.Layer):\n",
        "  \"\"\"\n",
        "  Implements the feedforward sublayer of the transformer block.\n",
        "\n",
        "  Parameters:\n",
        "  -----------\n",
        "  d_model: int\n",
        "      The number of expected features in the input and output.\n",
        "  dff: int\n",
        "      The number of neurons in the first Dense layer.\n",
        "  dropout_rate: float, optional (default=0.1)\n",
        "      The dropout rate to use.\n",
        "\n",
        "  Attributes:\n",
        "  -----------\n",
        "  seq: tf.keras.Sequential\n",
        "      The sequential model that applies the two Dense layers and Dropout.\n",
        "  add: tf.keras.layers.Add\n",
        "      The addition layer that adds the residual connection.\n",
        "  layer_norm: tf.keras.layers.LayerNormalization\n",
        "      The normalization layer applied to the output.\n",
        "\n",
        "  Methods:\n",
        "  --------\n",
        "  call(x):\n",
        "      Computes the feedforward sublayer on the input tensor x and returns the output.\n",
        "\n",
        "  \"\"\"\n",
        "  def __init__(self, d_model, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.seq = tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),\n",
        "      tf.keras.layers.Dense(d_model),\n",
        "      tf.keras.layers.Dropout(dropout_rate)\n",
        "    ])\n",
        "    self.add = tf.keras.layers.Add()\n",
        "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  def call(self, x):\n",
        "    \"\"\"\n",
        "    Passes the input tensor `x` through a feedforward network consisting of two\n",
        "    dense layers with `dff` hidden units and a `relu` activation function.\n",
        "    A `dropout_rate` is applied after the first dense layer to prevent overfitting.\n",
        "    The output of the feedforward network is added to the original input `x` via the\n",
        "    `Add()` layer. Finally, the output is normalized using the `LayerNormalization()` layer.\n",
        "\n",
        "    Args:\n",
        "        x (tf.Tensor): Input tensor with shape `(batch_size, seq_len, d_model)`.\n",
        "\n",
        "    Returns:\n",
        "        tf.Tensor: Output tensor with shape `(batch_size, seq_len, d_model)`.\n",
        "    \"\"\"\n",
        "    x = self.add([x, self.seq(x)])\n",
        "    x = self.layer_norm(x)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "LCZ_3CmiHcdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  \"\"\"\n",
        "  A single layer of the decoder in a transformer-based architecture.\n",
        "\n",
        "  Args:\n",
        "    d_model (int): The number of expected features in the input.\n",
        "    num_heads (int): The number of attention heads.\n",
        "    dff (int): The dimensionality of the feedforward network.\n",
        "    dropout_rate (float): The dropout rate to be applied.\n",
        "\n",
        "  Attributes:\n",
        "    causal_self_attention: An instance of the `CausalSelfAttention` layer.\n",
        "    cross_attention: An instance of the `CrossAttention` layer.\n",
        "    ffn: An instance of the `FeedForward` layer.\n",
        "    last_attn_scores: A tensor containing the last attention scores.\n",
        "\n",
        "  \"\"\"\n",
        "  def __init__(self,\n",
        "               *,\n",
        "               d_model,\n",
        "               num_heads,\n",
        "               dff,\n",
        "               dropout_rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    self.causal_self_attention = CausalSelfAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    self.ffn = FeedForward(d_model, dff)\n",
        "\n",
        "  def call(self, x, context):\n",
        "    \"\"\"\n",
        "    Forward pass of the `DecoderLayer`.\n",
        "\n",
        "    Args:\n",
        "      x (tf.Tensor): The input tensor of shape\n",
        "      `(batch_size, target_seq_len, d_model)`.\n",
        "      context (tf.Tensor): The context tensor of shape\n",
        "      `(batch_size, input_seq_len, d_model)`.\n",
        "\n",
        "    Returns:\n",
        "      The output tensor of the `DecoderLayer` of shape\n",
        "      `(batch_size, target_seq_len, d_model)`.\n",
        "\n",
        "    \"\"\"\n",
        "    x = self.causal_self_attention(x=x)\n",
        "\n",
        "    # Cache the last attention scores for plotting later\n",
        "    self.last_attn_scores = self.cross_attention.last_attn_scores\n",
        "\n",
        "    x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
        "    return x"
      ],
      "metadata": {
        "id": "GFlWA0-XHnEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  \"\"\"A decoder model for sequence to sequence learning.\n",
        "\n",
        "  This class implements a decoder layer for a transformer-based model used for sequence to sequence learning tasks. The decoder layer takes input embeddings, positional encodings, and attention masks as input, and returns the output of the decoder layer after applying a multi-head self-attention mechanism, followed by a cross-attention mechanism with the output from the encoder layers, and then applying a feed-forward neural network.\n",
        "\n",
        "  Attributes:\n",
        "    d_model (int): The number of output dimensions for each layer.\n",
        "    num_layers (int): The number of layers in the decoder.\n",
        "    pos_embedding (PositionalEmbedding): The positional embedding layer.\n",
        "    dropout (Dropout): A dropout layer.\n",
        "    dec_layers (list): A list of DecoderLayer objects.\n",
        "    last_attn_scores (ndarray): The attention scores from the last decoder layer.\n",
        "\n",
        "  Methods:\n",
        "    call(x, context): Implements the forward pass for the decoder layer.\n",
        "      Args:\n",
        "        x (ndarray): A tensor of shape (batch_size, target_seq_len), representing the input token IDs.\n",
        "        context (ndarray): A tensor of shape (batch_size, input_seq_len, d_model), representing the output from the encoder layers.\n",
        "      Returns:\n",
        "        ndarray: A tensor of shape (batch_size, target_seq_len, d_model), representing the output from the decoder layers.\n",
        "  \"\"\"\n",
        "  def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size,\n",
        "               dropout_rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size,\n",
        "                                             d_model=d_model)\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "    self.dec_layers = [\n",
        "        DecoderLayer(d_model=d_model, num_heads=num_heads,\n",
        "                     dff=dff, dropout_rate=dropout_rate)\n",
        "        for _ in range(num_layers)]\n",
        "\n",
        "    self.last_attn_scores = None\n",
        "\n",
        "  def call(self, x, context):\n",
        "    \"\"\"\n",
        "    Implements the forward pass for the decoder layer.\n",
        "\n",
        "    Args:\n",
        "      x (ndarray): A tensor of shape (batch_size, target_seq_len), representing the input token IDs.\n",
        "      context (ndarray): A tensor of shape (batch_size, input_seq_len, d_model), representing the output from the encoder layers.\n",
        "\n",
        "    Returns:\n",
        "      ndarray: A tensor of shape (batch_size, target_seq_len, d_model), representing the output from the decoder layers.\n",
        "    \"\"\"\n",
        "    # `x` is token-IDs shape (batch, target_seq_len)\n",
        "    x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x  = self.dec_layers[i](x, context)\n",
        "\n",
        "    self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
        "\n",
        "    # The shape of x is (batch_size, target_seq_len, d_model).\n",
        "    return x"
      ],
      "metadata": {
        "id": "L9j5QyPNH4Me"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the decoder.\n",
        "sample_decoder = Decoder(num_layers=4,\n",
        "                         d_model=512,\n",
        "                         num_heads=8,\n",
        "                         dff=2048,\n",
        "                         vocab_size=8000)\n",
        "\n",
        "output = sample_decoder(\n",
        "    x=en,\n",
        "    context=pt_emb)\n",
        "\n",
        "# Print the shapes.\n",
        "print(en.shape)\n",
        "print(pt_emb.shape)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "id": "DWzsFqNgIMWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "  \"\"\"\n",
        "  A transformer model that consists of an encoder, a decoder and a final dense layer.\n",
        "\n",
        "  Args:\n",
        "    num_layers (int): Number of layers in both the encoder and decoder.\n",
        "    d_model (int): Hidden size of the model.\n",
        "    num_heads (int): Number of attention heads used in the model.\n",
        "    dff (int): Size of the feedforward layer in the encoder and decoder.\n",
        "    input_vocab_size (int): Size of the vocabulary of the input.\n",
        "    target_vocab_size (int): Size of the vocabulary of the target.\n",
        "    dropout_rate (float): Dropout rate applied to the output of each sub-layer.\n",
        "\n",
        "  Attributes:\n",
        "    encoder (Encoder): An instance of the Encoder class.\n",
        "    decoder (Decoder): An instance of the Decoder class.\n",
        "    final_layer (Dense): A Dense layer that converts the final transformer output to output token probabilities.\n",
        "\n",
        "  Methods:\n",
        "    call(inputs): Forward pass of the transformer model.\n",
        "\n",
        "  Returns:\n",
        "    logits (tf.Tensor): Output tensor of the final dense layer. Shape (batch_size, target_len, target_vocab_size).\n",
        "  \"\"\"\n",
        "  def __init__(self, *, num_layers, d_model, num_heads, dff,\n",
        "               input_vocab_size, target_vocab_size, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.decoder = Decoder(num_layers=num_layers, d_model=d_model,\n",
        "                           num_heads=num_heads, dff=dff,\n",
        "                           vocab_size=target_vocab_size,\n",
        "                           dropout_rate=dropout_rate)\n",
        "\n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    \"\"\"\n",
        "    Forward pass of the transformer model.\n",
        "\n",
        "    Args:\n",
        "      inputs (tuple): A tuple of two tensors. The first tensor is the context input tensor of shape (batch_size, context_len).\n",
        "                      The second tensor is the target input tensor of shape (batch_size, target_len).\n",
        "\n",
        "    Returns:\n",
        "      logits (tf.Tensor): Output tensor of the final dense layer. Shape (batch_size, target_len, target_vocab_size).\n",
        "    \"\"\"\n",
        "\n",
        "    # To use a Keras model with `.fit` you must pass all your inputs in the\n",
        "    # first argument.\n",
        "    x  = inputs\n",
        "\n",
        "    #context = self.encoder(context)  # (batch_size, context_len, d_model)\n",
        "\n",
        "    x = self.decoder(x)  # (batch_size, target_len, d_model)\n",
        "\n",
        "    # Final linear layer output.\n",
        "    logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)\n",
        "\n",
        "    try:\n",
        "      # Drop the keras mask, so it doesn't scale the losses/metrics.\n",
        "      # b/250038731\n",
        "      del logits._keras_mask\n",
        "    except AttributeError:\n",
        "      pass\n",
        "\n",
        "    # Return the final output and the attention weights.\n",
        "    return logits"
      ],
      "metadata": {
        "id": "flmHgGz8IUFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_layers = 4\n",
        "d_model = 128\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "dropout_rate = 0.1"
      ],
      "metadata": {
        "id": "gImmN6hEIgo9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = Transformer(\n",
        "    num_layers=num_layers,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    dff=dff,\n",
        "    input_vocab_size=tokenizers.pt.get_vocab_size().numpy(),\n",
        "    target_vocab_size=tokenizers.en.get_vocab_size().numpy(),\n",
        "    dropout_rate=dropout_rate)"
      ],
      "metadata": {
        "id": "N-ha2MJmIjxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.summary()"
      ],
      "metadata": {
        "id": "HO3PCHcHInXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  \"\"\"\n",
        "  Custom learning rate schedule that implements the learning rate function\n",
        "  described in the original Transformer paper. The learning rate is increased\n",
        "  linearly for the first `warmup_steps` training steps, and then decreased\n",
        "  proportionally to the inverse square root of the step number.\n",
        "\n",
        "  Args:\n",
        "    d_model (int): the dimensionality of the model.\n",
        "    warmup_steps (int): the number of steps taken to increase the learning rate\n",
        "      linearly. Default is 4000.\n",
        "\n",
        "  Attributes:\n",
        "    d_model (float): the dimensionality of the model as a float.\n",
        "    warmup_steps (int): the number of steps taken to increase the learning rate\n",
        "      linearly.\n",
        "\n",
        "  Methods:\n",
        "    __call__(step): returns the learning rate at the given step.\n",
        "\n",
        "  Returns:\n",
        "    The learning rate at the given step.\n",
        "  \"\"\"\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    \"\"\"\n",
        "    Returns the learning rate at the given step.\n",
        "\n",
        "    Args:\n",
        "      step (int): the current training step.\n",
        "\n",
        "    Returns:\n",
        "      The learning rate at the given step as a float32 tensor.\n",
        "    \"\"\"\n",
        "    step = tf.cast(step, dtype=tf.float32)\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "metadata": {
        "id": "ZBytL287IsRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
        "                                     epsilon=1e-9)"
      ],
      "metadata": {
        "id": "BlicjSDzIsv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def masked_loss(label, pred):\n",
        "  \"\"\"\n",
        "  Calculates the masked sparse categorical cross-entropy loss between the true labels and predicted labels.\n",
        "\n",
        "  Args:\n",
        "      label: A tensor of shape (batch_size, seq_length) containing the true labels.\n",
        "      pred: A tensor of shape (batch_size, seq_length, target_vocab_size) containing the predicted labels.\n",
        "\n",
        "  Returns:\n",
        "      A scalar tensor representing the masked loss value.\n",
        "\n",
        "  \"\"\"\n",
        "  mask = label != 0\n",
        "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "  loss = loss_object(label, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss.dtype)\n",
        "  loss *= mask\n",
        "\n",
        "  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
        "  return loss"
      ],
      "metadata": {
        "id": "oeCXNP-hIvC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def masked_accuracy(label, pred):\n",
        "  \"\"\"\n",
        "  Calculates the masked accuracy between the true labels and predicted labels.\n",
        "\n",
        "  Args:\n",
        "      label: A tensor of shape (batch_size, seq_length) containing the true labels.\n",
        "      pred: A tensor of shape (batch_size, seq_length, target_vocab_size) containing the predicted labels.\n",
        "\n",
        "  Returns:\n",
        "      A scalar tensor representing the masked accuracy value.\n",
        "\n",
        "  \"\"\"\n",
        "  pred = tf.argmax(pred, axis=2)\n",
        "  label = tf.cast(label, pred.dtype)\n",
        "  match = label == pred\n",
        "\n",
        "  mask = label != 0\n",
        "\n",
        "  match = match & mask\n",
        "\n",
        "  match = tf.cast(match, dtype=tf.float32)\n",
        "  mask = tf.cast(mask, dtype=tf.float32)\n",
        "  return tf.reduce_sum(match)/tf.reduce_sum(mask)"
      ],
      "metadata": {
        "id": "VfrjqxP7I-5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.compile(\n",
        "    loss=masked_loss,\n",
        "    optimizer=optimizer,\n",
        "    metrics=[masked_accuracy])"
      ],
      "metadata": {
        "id": "UMkzTRd9I_dP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.fit(train_batches,\n",
        "                epochs=20,\n",
        "                validation_data=val_batches)"
      ],
      "metadata": {
        "id": "nZHdmO1EJBhu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}